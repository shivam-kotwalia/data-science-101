{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "An Artificial Neural Network (ANN) is a computational model that is inspired by the way biological neural networks in the human brain process information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Neuron"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The basic unit of computation in a neural network is the neuron, often called a node or unit. It receives input from some other nodes, or from an external source and computes an output. Each input has an associated weight (w), which is assigned on the basis of its relative importance to other inputs. The node applies a function f (defined below) to the weighted sum of its inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-3-42-21-am.png?w=1024&h=547\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The above network takes numerical \n",
    "inputs X1 and X2 and has \n",
    "weights w1 and w2 associated with those inputs. \n",
    "\n",
    "Additionally, there is another input 1 with weight b (called the Bias) associated with it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The function f is non-linear and is called the Activation Function. \n",
    "The purpose of the activation function is to introduce non-linearity into the output of a neuron. \n",
    "This is important because most real world data is non linear and we want neurons to learn these non linear representations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Every activation function (or non-linearity) takes a single number and performs a certain fixed mathematical operation on it. There are several activation functions you may encounter in practice:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "    Sigmoid: takes a real-valued input and squashes it to range between 0 and 1\n",
    "\n",
    "σ(x) = 1 / (1 + exp(−x))\n",
    "\n",
    "    tanh: takes a real-valued input and squashes it to the range [-1, 1]\n",
    "\n",
    "tanh(x) = 2σ(2x) − 1\n",
    "\n",
    "    ReLU: ReLU stands for Rectified Linear Unit. It takes a real-valued input and thresholds it at zero (replaces negative values with zero)\n",
    "\n",
    "f(x) = max(0, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-08-at-11-53-41-am.png?w=1493\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Importance of Bias: The main function of Bias is to provide every node with a trainable constant value (in addition to the normal inputs that the node receives). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The feedforward neural network was the first and simplest type of artificial neural network devised. It contains multiple neurons (nodes) arranged in layers. Nodes from adjacent layers have connections or edges between them. All these connections have weights associated with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-4-19-50-am.png?w=768&h=568\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A feedforward neural network can consist of three types of nodes:\n",
    "\n",
    "    Input Nodes – The Input nodes provide information from the outside world to the network and are together referred to as the “Input Layer”. No computation is performed in any of the Input nodes – they just pass on the information to the hidden nodes.\n",
    "    \n",
    "    Hidden Nodes – The Hidden nodes have no direct connection with the outside world (hence the name “hidden”). They perform computations and transfer information from the input nodes to the output nodes. A collection of hidden nodes forms a “Hidden Layer”. While a feedforward network will only have a single input layer and a single output layer, it can have zero or multiple Hidden Layers.\n",
    "    \n",
    "    Output Nodes – The Output nodes are collectively referred to as the “Output Layer” and are responsible for computations and transferring information from the network to the outside world.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In a feedforward network, the information moves in only one direction – forward – from the input nodes, through the hidden nodes (if any) and to the output nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A Multi Layer Perceptron (MLP) contains one or more hidden layers (apart from one input and one output layer).  While a single layer perceptron can only learn linear functions, a multi layer perceptron can also learn non – linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://ujwlkarn.files.wordpress.com/2016/08/ds.png?w=1024\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-Propagation Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "often abbreviated as BackProp is one of the several ways in which an artificial neural network (ANN) can be trained. It is a supervised training scheme, which means, it learns from labeled training data (there is a supervisor, to guide its learning).\n",
    "\n",
    "To put in simple terms, BackProp is like “learning from mistakes“. The supervisor corrects the ANN whenever it makes mistakes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Initially all the edge weights are randomly assigned. For every input in the training dataset, the ANN is activated and its output is observed. This output is compared with the desired output that we already know, and the error is “propagated” back to the previous layer. This error is noted and the weights are “adjusted” accordingly. This process is repeated until the output error is below a predetermined threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/faq/visual-backpropagation/nonconvex-cost.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/faq/visual-backpropagation/backpropagation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-11-53-06-pm.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Detailed info - http://peterroelants.github.io/posts/neural_network_implementation_part01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. OpenCV\n",
    "2. skimage\n",
    "3. PIL\n",
    "4. keras"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Error = (ya - yp)\n",
    "to reduce the Error and make it ZERO "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "E = ya - yp \n",
    "E = (w1*x1 + b1 - yp)\n",
    "Wn = Wo -alpha*(dE/dW)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
